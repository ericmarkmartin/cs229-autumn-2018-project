>>> from src import vectorize
>>> from sklearn.svm import SVC
>>> from sklearn.metrics import classification_report, confusion_matrix
>>> target_names = ['PERIOD', 'QMARK', 'EXPOINT']
>>> X, y = vectorize.tokens_to_bag_of_words('./data/processed/merged_tok.txt', one_hot
_y=False)
>>> train, dev, test = vectorize.train_dev_test_split(X, y, 0.05, 0.05)
>>> len(train)
2
>>> len(train[0])
42652
>>> len(dev)
2
>>> len(dev[0])
2370
>>> import numpy as np
>>> np.random.sample(
...
KeyboardInterrupt
>>> help(np.random.sample)

>>> clf = SVC(gamma='auto')
>>> train
(array([[1, 0, 1, ..., 0, 3, 0],
       [0, 0, 0, ..., 0, 1, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 2, 0],
       [3, 2, 2, ..., 0, 4, 0],
       [3, 1, 1, ..., 0, 2, 0]]), array([[0],
       [0],
       [1],
       ...,
       [1],
       [0],
       [0]]))
>>> x_train, y_train = train
>>> clf
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
>>> clf = SVC(gamma='auto', probability=True)
>>> clf = SVC(gamma='auto', probability=True, random_state=229)
>>> clf
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=True, random_state=229, shrinking=True,
  tol=0.001, verbose=False)
>>> np.random.choice(x_train, size=5000, replace=False)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "mtrand.pyx", line 1122, in mtrand.RandomState.choice
ValueError: a must be 1-dimensional
>>>
>>> sampleInd = np.random.choice(x_train.shape[0], size=(5000,))
>>> sampleInd
array([22251,  1823, 28742, ..., 20248, 22451, 34958])
>>> sample = x
KeyboardInterrupt
>>> x_train_sample = x_train[sample]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'sample' is not defined
>>> x_train_sample = x_train[sampleInd]
>>> len(x_train_sample)
5000
>>> clf
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=True, random_state=229, shrinking=True,
  tol=0.001, verbose=False)
>>> y_train_sample = y_train[sampleInd]
>>> y_train_sample
array([[1],
       [0],
       [0],
       ...,
       [0],
       [0],
       [0]])
>>> y_train_sample = y_train_sample.unravel()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'numpy.ndarray' object has no attribute 'unravel'
>>> y_train_sample = y_train_sample.ravel()
>>> y_train_sample
array([1, 0, 0, ..., 0, 0, 0])
>>> clf.fit(x_train_sample, y_train_sample)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=True, random_state=229, shrinking=True,
  tol=0.001, verbose=False)
>>> x_dev, y_dev = dev
>>> y_dev
array([[0],
       [0],
       [2],
       ...,
       [0],
       [2],
       [0]])
>>> y_dev = y_dev.ravel()
>>> y_dev
array([0, 0, 2, ..., 0, 2, 0])
>>> y_dev == 1
array([False, False, False, ..., False, False, False])
>>> np.all(y_dev == 1)
False
>>> y_pred = clf.predict(x_dev)
>>> print(classification_report(y_dev, y_pred, target_names=target_names))
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
              precision    recall  f1-score   support

      PERIOD       0.82      1.00      0.90      1942
       QMARK       0.00      0.00      0.00       198
     EXPOINT       0.00      0.00      0.00       230

   micro avg       0.82      0.82      0.82      2370
   macro avg       0.27      0.33      0.30      2370
weighted avg       0.67      0.82      0.74      2370

>>> y_prob = clf.predict_proba(x_dev)
>>> y_prob[:10]
array([[0.97780246, 0.02013661, 0.00206093],
       [0.82680428, 0.15225507, 0.02094065],
       [0.77486806, 0.06975255, 0.15537938],
       [0.72192732, 0.07346073, 0.20461195],
       [0.77301909, 0.07278931, 0.1541916 ],
       [0.67958066, 0.04784318, 0.27257616],
       [0.8650564 , 0.08060489, 0.05433871],
       [0.75810811, 0.08485472, 0.15703717],
       [0.94508614, 0.02312318, 0.03179067],
       [0.80157037, 0.08568095, 0.11274868]])
>>> np.max(y_prob)
0.9999273960622467
>>> np.max(y_prob, axis=0)
array([0.9999274 , 0.92858192, 0.82411938])
>>> np.bin_count(y_prob)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: module 'numpy' has no attribute 'bin_count'
>>> np.bin_count(y_prob)
KeyboardInterrupt
>>> np.savetxt(y_prob)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: savetxt() missing 1 required positional argument: 'X'
>>> np.savetxt('/tmp/probs', y_prob)
>>> np.savetxt('/tmp/dev', x_dev)
>>> len(y_prob)
2370
>>> len(x_dev)
2370
>>> x_dev.dtype
dtype('int64')
>>> help(np.savetxt)

>>> np.savetxt('/tmp/dev.txt', x_dev.astype(int))
>>> np.savetxt('/tmp/dev.txt', x_dev.astype(int), fmt='u')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/numpy/lib/npyio.py", line 1367, in savetxt
    raise error
ValueError: fmt has wrong number of % formats:  u
>>> np.savetxt('/tmp/dev.txt', x_dev.astype(int), fmt='%1u')
>>> from sklearn.linear_model import SGDClassifier
>>> text_clf_svm = SGDClassifier()
>>> text_clf_svm.fit(x_train, y_train)
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.
  FutureWarning)
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
^CTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py", line 743, in fit
    sample_weight=sample_weight)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py", line 596, in _fit
    classes, sample_weight, coef_init, intercept_init)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py", line 521, in _partial_fit
    accept_large_sparse=False)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py", line 756, in check_X_y
    estimator=estimator)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py", line 573, in check_array
    allow_nan=force_all_finite == 'allow-nan')
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py", line 49, in _assert_all_finite
    if is_float and np.isfinite(X.sum()):
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/numpy/core/_methods.py", line 36, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial)
KeyboardInterrupt
>>> text_clf_svm.fit(x_train, y_train.ravel())
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.
  FutureWarning)
SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,
       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
       power_t=0.5, random_state=None, shuffle=True, tol=None,
       validation_fraction=0.1, verbose=0, warm_start=False)
>>> text_clf_svm.
KeyboardInterrupt
>>> y_pred = text_clf_svm.predict(x_dev)
>>> print(classification_report(y_dev, y_pred, target_names=target_names))
              precision    recall  f1-score   support

      PERIOD       0.85      0.99      0.92      1942
       QMARK       0.73      0.26      0.39       198
     EXPOINT       0.68      0.13      0.22       230

   micro avg       0.85      0.85      0.85      2370
   macro avg       0.76      0.46      0.51      2370
weighted avg       0.83      0.85      0.80      2370

>>> help(confusion_matrix)

>>> confusion_matrix(y_dev, y_pred)
array([[1922,   10,   10],
       [ 142,   52,    4],
       [ 191,    9,   30]])
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> TfidfVectorizer(input='filename')
TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='filename',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> tfidf = TfidfTransformer()
>>> x_train
array([[1, 0, 1, ..., 0, 3, 0],
       [0, 0, 0, ..., 0, 1, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 2, 0],
       [3, 2, 2, ..., 0, 4, 0],
       [3, 1, 1, ..., 0, 2, 0]])
>>> tfidf.fit_transform(x_train)
^CTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/base.py", line 462, in fit_transform
    return self.fit(X, **fit_params).transform(X)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/feature_extraction/text.py", line 1266, in transform
    X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py", line 527, in check_array
    array = np.asarray(array, dtype=dtype, order=order)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/numpy/core/numeric.py", line 501, in asarray
    return array(a, dtype, copy=False, order=order)
KeyboardInterrupt
>>> x_tfidf = tfidf.fit_transform(x_train)
>>> x_tfidf.shape
(42652, 20005)
>>> x_train.shape
(42652, 20005)
>>> text_clf_svm_2 = Tex
KeyboardInterrupt
>>> from sklearn.pipeline import Pipeline
>>> text_clf_tf = Pipeline([('tfidf', TfidfTransformer()),
... ('clf-svm', SGDClassifier())]
...
... )
>>> text_clf_tf
text_clf_tf
>>> text_clf_tf.fit(x_train, y_train)
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.
  FutureWarning)
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
Pipeline(memory=None,
     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf-svm', SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
       l1_ratio=0.15, learning_rate='optimal', loss='hin...m_state=None, shuffle=True, tol=None,
       validation_fraction=0.1, verbose=0, warm_start=False))])
>>> text_clf_tf.fit(x_train, y_train.ravel())
/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.
  FutureWarning)
Pipeline(memory=None,
     steps=[('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf-svm', SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
       l1_ratio=0.15, learning_rate='optimal', loss='hin...m_state=None, shuffle=True, tol=None,
       validation_fraction=0.1, verbose=0, warm_start=False))])
>>> y_pred = text_clf_tf.predict(x_test)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'x_test' is not defined
>>> y_pred = text_clf_tf.predict(x_train)
^CTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/metaestimators.py", line 118, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/pipeline.py", line 331, in predict
    Xt = transform.transform(Xt)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/feature_extraction/text.py", line 1266, in transform
    X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/sklearn/utils/validation.py", line 527, in check_array
    array = np.asarray(array, dtype=dtype, order=order)
  File "/Users/ericmarkmartin/.virtualenvs/cs229-autumn-2018-project-3R6jIYo2/lib/python3.6/site-packages/numpy/core/numeric.py", line 501, in asarray
    return array(a, dtype, copy=False, order=order)
KeyboardInterrupt
>>> y_pred = text_clf_tf.predict(x_dev)
>>> np.mean(y_pred ==
KeyboardInterrupt
>>> y_dev
array([0, 0, 2, ..., 0, 2, 0])
>>> np.mean(y_pred == y_dev)
0.839240506329114
>>> print(classification_report(y_dev, y_pred, target_names=target_names))
              precision    recall  f1-score   support

      PERIOD       0.84      1.00      0.91      1942
       QMARK       0.78      0.21      0.33       198
     EXPOINT       0.87      0.06      0.11       230

   micro avg       0.84      0.84      0.84      2370
   macro avg       0.83      0.42      0.45      2370
weighted avg       0.84      0.84      0.79      2370

>>> confusion_matrix(y_dev, y_pred)
array([[1934,    6,    2],
       [ 156,   42,    0],
       [ 211,    6,   13]])
>>>
